# Lectures on Natural Language Processing

The purpose of this repostitory is to share the materials in a graduate-level course on NLP that I teach regularly. I will make an effort to refresh it occasionally as long as I teach it. 

## Syllabus (2023 Spring Edition)

| **#** | **Topic**                                       | **Optional Reading** |
|---------|--------------------------------------------------|----------------------|
| 1       | [General introduction](lectures/lecture1.pdf)                             |                      |
| 2       | [Classification](lectures/lecture2.pdf)                                   |   [Steepest descent](http://karlstratos.com/notes/descent.pdf)                   |
| 3       | [Linear classifiers](lectures/lecture3.pdf)                               |   [Linear separability (Jupyter Notebook)](https://colab.research.google.com/drive/1co9mZ79P7o5cbdpSyaVqFcXkgQFK5jh1#scrollTo=KBeLjgzRHSu_&sandboxMode=true)                   |
| 4       | [Introduction to deep learning](lectures/lecture4.pdf)                    |   [Backpropagation](http://karlstratos.com/notes/backprop.pdf), [adaptive learning](http://karlstratos.com/notes/adaptive.pdf)         |
| 5       | [Neural architectures for language processing](lectures/lecture5.pdf)     |   [Transformers](http://karlstratos.com/notes/transformer17.pdf)                   |
| 6       | [Language models](lectures/lecture6.pdf)                                  |                      |
| 7       | [Conditional language models](lectures/lecture7.pdf)                      |                      |
| 8       | [Pretrained language models](lectures/lecture8.pdf)                       |   [BERT finetuning (Jupyter Notebook)](https://colab.research.google.com/drive/1us2wdDcnlN4TE13OFsV9nic6FA0JpLPG#scrollTo=KBeLjgzRHSu_&sandboxMode=true)                   |
| 9       | [Prompting large language models](lectures/lecture9.pdf)                  |                      |
| 10      | [Retrieval-augmented models](lectures/lecture10.pdf)                       |   [Noise contrastive estimation](http://karlstratos.com/notes/nce.pdf), [search indexes](http://karlstratos.com/notes/indexes.pdf)                   |
| 11      | [Structured prediction: HMMs and PCFGs](lectures/lecture11.pdf)            |                      |
| 12      | [Structured prediction: CRFs](lectures/lecture12.pdf)                      |   [Variable elimination](http://karlstratos.com/notes/graphical_models.pdf)                   |
| 13      | [Latent-variable generative models](lectures/lecture13.pdf)                |   [Gaussian](http://karlstratos.com/notes/gaussian.pdf), [VAEs](http://karlstratos.com/notes/varinfo.pdf), [LVGMs](http://karlstratos.com/notes/facts_latent.pdf)                    |
| 14      | [Diffusion models, coreference resolution, review](lectures/lecture14.pdf) |   [Diffusion models](http://karlstratos.com/notes/diffusion.pdf)                   |

## Acknowledgement

I've spent many hours to design and create most of the materials. But no class stands on its own. I stole many pedagogical ideas from my colleagues. The general introduction was inspired by the NLP class taught by Danqi Chen and Karthik Narasimhan; Dan Edminston pointed me to the ambiguous headline "British Left Waffles on Falklands"; the wonderful cartoon-style panel illustrating a debate between Chomsky and Hinton was created by Nathan Srebro; the animation illustrating stochatic gradient descent was created by Greg Shakhnarovich. 
